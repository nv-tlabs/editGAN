<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-53775284-6"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-53775284-6');
</script>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<link rel="icon" href="https://nv-tlabs.github.io/images/logo_hu2fe6632db44d28c9b9d53edd3914c1d6_112452_0x70_resize_lanczos_2.png">
<link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">


</head>

<style type="text/css">
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight:300;
        font-size:18px;
        margin-left: auto;
        margin-right: auto;
        width: 1100px;
    }
    .topnav {
      background-color: #eeeeee;
      overflow: hidden;
    }

    .topnav div {
      max-width: 1070px;
      margin: 0 auto;
    }

    .topnav a {
      display: inline-block;
      color: black;
      text-align: center;
      vertical-align: middle;
      padding: 16px 16px;
      text-decoration: none;
      font-size: 16px;
    }

    .topnav img {
      width: 100%;
      margin: 0.2em 0px 0.3em 0px;
    }
    .venue {
      color: black;
      text-align: center;
    }
    #bibtex pre {
        font-size: 14px;
        background-color: #eee;
        padding: 16px;
    }
    h1 {
        font-weight:300;
        margin: 0.4em;
    }

    p {
        margin: 0.2em;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }
  .paper-btn {
    position: relative;
    text-align: center;

    display: inline-block;
    margin: 8px;
    padding: 8px 8px;

    border-width: 0;
    outline: none;
    border-radius: 2px;
    
    background-color: #76B900;
    color: black !important;
    font-size: 20px;
    width: 100px;
    font-weight: 600;
  }
  .paper-btn-parent {
      display: flex;
      justify-content: center;
      margin: 16px 0px;
  }
  .paper-btn:hover {
      opacity: 0.85;
  }
  .material-icons {
      vertical-align: -6px;
  }
/*    a:link,a:visited
    {
        color: #1367a7;
        text-decoration: none;
    }
    a:hover {
        color: #208799;
    }*/

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
                15px 15px 0 0px #fff, /* The fourth layer */
                15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
                20px 20px 0 0px #fff, /* The fifth layer */
                20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
                25px 25px 0 0px #fff, /* The fifth layer */
                25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr
    {
        margin: 0;
        border: 0;
        height: 1.5px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }
</style>

<html>
  <head>
        <title>EditGAN</title>
  </head>

  <body>
    <div class="topnav" id="myTopnav">
      <div>
        <a href="https://www.nvidia.com/"
          ><img width="100%" src="resources/nvidia.svg"
        /></a>
        <a href="https://nv-tlabs.github.io/"
          ><strong>Toronto AI Lab</strong></a
        >

      </div>
    </div>
    <br>
    <center>
    <span style="font-size:42px">EditGAN: High-Precision Semantic Image Editing</span>
    </center>

    <br>
      <table align=center width=700px>
       <tr> 

  


        <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="http://www.cs.toronto.edu/~linghuan/">Huan Ling *</a><sup>1,2,3</sup></span>
        </center>
        </td>

              <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="https://nv-tlabs.github.io/author/karsten-kreis/">Karsten Kreis *</a><sup>1</sup></span>
        </center>
        </td>



        <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="https://scholar.google.ca/citations?user=8q2ISMIAAAAJ&hl=en">Daiqing Li </a><sup>1</sup></span>
        </center>
        </td>





       
     </tr>
    </table>


      <table align=center width=700px>

    <tr>
              <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="https://seung-kim.github.io/seungkim/">Seung Wook Kim </a><sup>1,2,3</sup></span>
        </center>
        </td>
          
                <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="https://groups.csail.mit.edu/vision/torralbalab/"> Antonio Torralba</a><sup>4</sup></span>
        </center>
        </td>

      <td align=center width=100px>
            <center>
            <span style="font-size:20px"><a href="http://www.cs.toronto.edu/~fidler/">Sanja Fidler</a><sup>1,2,3</sup></span>
            </center>
        </td>

       
     </tr>
    </table>



    <br>
    <table align=center width=900px>
       <tr>
              <td align=center width=100px>
            <center>
            <span style="font-size:20px"><sup>1</sup>NVIDIA</span>
            </center>
        </td>


        <td align=center width=100px>
        <center>

        <span style="font-size:20px"><sup>2</sup>University of Toronto</span>
        </center>
        </td>
        <td align=center width=100px>
            <center>
            <span style="font-size:20px"><sup>3</sup>Vector Institute</span>
            </center>
        </td>
        
    
     <td align=center width=100px>
            <center>
            <span style="font-size:20px"><sup>4</sup>MIT</span>
            </center>
        </td>
     </tr>
    </table>

    <br>
    <p class="venue"><b>NeurIPS 2021</b></p>
<center>
            <div class="paper-btn-parent">
            <a class="paper-btn" href="https://arxiv.org/abs/2111.03186">
                <span class="material-icons">description</span> 
                 Paper
            </a>   <a class="paper-btn" href="./bib.txt">
                <span class="material-icons">description</span> 
                 BibTex
            </a>
            </div>
</center>
<br>
               <hr><hr>





          <!-- <center><h1>Abstract</h1></center> -->
            <table align=center width=900px>
                        <td width=600px>
                      <center>
                             <img src = "./resources/editgan_pipeline.png" width="1050px"></img>
                                                 <span style="font-size:15px;font-style:italic">  (1) EditGAN builds on a GAN framework that jointly models images and their semantic segmentations. (2 & 3) Users can modify segmentation masks, based on which we perform optimization in the GAN’s latent space to realize the edit. (4) Users can perform editing simply by applying previously learnt editing vectors and manipulate images at interactive rates.</span>

                    </center>

                    </td>
                         </tr>
           
                </tr>
            </table>
            <table align=center width=900px></table>

                <tr>
                    <td width=600px>
                        <br>
                        <p align="justify" style="font-size: 18px">
                       Generative adversarial networks (GANs) have recently found applications in image editing. However, most GAN-based image editing methods often require large-scale datasets with semantic segmentation annotations for training, only provide high level control, or merely interpolate between different images. Here, we propose EditGAN, a novel method for high-quality, high-precision semantic image editing, allowing users to edit images by modifying their highly detailed part segmentation masks, e.g., drawing a new mask for the headlight of a car. EditGAN builds on a GAN framework that jointly models images and their semantic segmentations (<a href="https://nv-tlabs.github.io/datasetGAN/">DatasetGAN</a>), requiring only a handful of labeled examples – making it a scalable tool for editing. Specifically, we embed an image into the GAN’s latent space and perform conditional latent code optimization according to the segmentation edit, which effectively also modifies the image. To amortize optimization, we find “editing vectors” in latent space that realize the edits. The framework allows us to learn an arbitrary number of editing vectors, which can then be directly applied on other images at interactive rates. We experimentally show that EditGAN can manipulate images with an unprecedented level of detail and freedom, while preserving full image quality. We can also easily combine multiple edits and perform plausible edits beyond EditGAN’s training data. We demonstrate EditGAN on a wide variety of image types and quantitatively outperform several previous editing methods on standard editing benchmark tasks.
                       <br><br>
                       EditGAN is the first GAN-driven image editing framework, which simultaneously <i>(i)</i> offers very high-precision editing, <i>(ii)</i> requires only very little annotated training data (and does not rely on external classifiers), <i>(iii)</i> can be run interactively in real time, <i>(iv)</i> allows for straightforward compositionality of multiple edits, <i>(v)</i> and works on real embedded, GAN-generated, and even out-of-domain images.
                        </p> </td>
      <br>      <br>
        <hr><hr>


 <center><h1>News</h1></center>

<ul>
<li>[November 2021] Project released on <a href=" https://arxiv.org/abs/2111.03186">arXiv</a>!</li>
<li>[September 2021] Paper accepted at NeurIPS 2021!</li>
</ul>
<hr><hr>

 <center><h1>Demos</h1></center>

                             <center>
                              <td width=300px>
                         <img src = "./resources/demo2.gif" width="500px" height="300px"></img>

                         <img src = "./resources/demo.gif" width="500px" height="300px"></img><br>
                                           <span style="font-size:14px">   <i>Left:</i> The video showcases EditGAN in an interacitve demo tool. <i>Right:</i> The video showcases EditGAN where we apply multiple edits and exploit pre-defined editing vectors. </span><br><br>

                                                  <img src = "./resources/demo_interp.gif" width="450px" height="350px"></img>

                         <img src = "./resources/demo_cross.gif" width="350px" height="350px"></img><br>
                                    <span style="font-size:14px">   <i>Left:</i> The video shows interpolation of editing vectors. <i>Right:</i> The video shows the result of applying EditGAN editing vectors on out-of-domain images. </span>
                                     

                         <br>
                    </center>

                    </td>
                </tr>
      
            </table>

          <br>
               <hr><hr>





          <center><h1>Results</h1></center>

          <table align=center width=900px>
              <tr>
                   <center>
                        <a href="./resources/results1.png"><img src = "./resources/results1.png" width="900px"></img></a><br>
                    </center>
                 
                </tr>
            <tr>
                <center>
                    <span style="font-size:14px">
                         Examples of segmentation-driven edits with EditGAN. <i>Blue boxes:</i> Original images. <i>Orange boxes:</i> Zoom-in view.
                        </span>
                    </center>
            </tr>
          
             <tr>
                        <center>
                        <a href="./resources/results2.png"><img src = "./resources/results2.png" width="900px"></img></a><br>
                    </center>
                </tr>
            <tr>
                <center>
                    <span style="font-size:14px">
                        We combine multiple edits. <i>Blue boxes:</i> Original images. <i>Edits in detail:</i> <i>Second row, first person:</i> open eyes, add hair, add mustache. <i>Second person:</i> smile, look left. <i>Third row, first car:</i> remove mirror, remove door handle, shrink wheels. <i>Second car:</i> remove license plate, enlarge wheels. <i>Third row, bird:</i> longer beak, bigger belly, head up. <i>Third row, cat:</i> open mouth, bigger ears, bigger eyes
                        </span>
                    </center>
            </tr>
             <br><br>


                  <tr>
                   <center>
                        <a href="./resources/results3.png"><img src = "./resources/results3.png" width="900px"></img></a><br>
                    </center>
                </tr>
      

            <tr>
                <center>
                    <span style="font-size:14px">
                     We showcase high-precision editing by wheel/spoke rotation. <i>First row:</i> Image and mask pair to learn editing vector. Images are images before editing and after editing. Segmentation masks are before editing and target segmentation mask after manual modification. <i>Second to fourth rows:</i> Applying the learnt edit on new images.

                        </span>
                    </center>
            </tr>
            

           
          </table>
 
<br>
<hr><hr>







            <table align=center width=800>

             <center><h1>Paper</h1></center>
                <tr>
                  <td><a href=""><img style="height:330px; border: solid; border-radius:30px;" src="./resources/paper.png"/></a></td>

                  <td><span><b>EditGAN:<br>High-Precision Semantic Image Editing</b></span><br><br>
                   Huan Ling*, Karsten Kreis*,  Daiqing Li,  <br> Seung Wook Kim, Antonio Torralba, Sanja Fidler
<br><br><i> * Authors contributed equally</i><br><br>
 <!-- <span style="background-color:#00FEFE">  Neurips'21 </span> -->
 <span><b> NeurIPS'21 </b></span><br><br>
                              
                    <span style="font-size:18px">
                      <a href="https://arxiv.org/abs/2111.03186" target="_blank">[Paper]</a></
                           <span style="font-size:18px"> &nbsp;&nbsp;&nbsp;&nbsp;  <a href="" target="_blank">[Code and Interactive Editing Tool]</a> (coming soon)</span>

                <br>
                <br>
                <p>For feedback and questions please reach out to <a href="mailto:huling@nvidia.com">Huan Ling</a> and <a href="mailto:kkreis@nvidia.com">Karsten Kreis</a>.</p>


                    </td>

  

              </tr>
               
                
                
   
              <tr>
                  <td colspan="5" style="font-size: 14px">
    
                  </td>
              </tr>



            </table>
            <br>

            <br>



        <hr><hr>

 <center><h1>Citation</h1></center>
<section id="bibtex">

      <p style="line-height: 1.75em; text-align: left;">If you find this work useful for your research, please consider citing it as:</p>
      <pre><code>@inproceedings{ling2021editgan,
  title = {EditGAN: High-Precision Semantic Image Editing}, 
  author = {Huan Ling and Karsten Kreis and Daiqing Li and Seung Wook Kim and Antonio Torralba and Sanja Fidler},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2021}
}
</code></pre>

See prior work on efficient semantic segmentation using GANs, which EditGAN builds on:  <br>
<a href="https://nv-tlabs.github.io/datasetGAN/">DatasetGAN</a>

     <pre><code>@inproceedings{zhang21,
  title={DatasetGAN: Efficient Labeled Data Factory with Minimal Human Effort},
  author={Zhang, Yuxuan and Ling, Huan and Gao, Jun and Yin, Kangxue and Lafleche, 
  Jean-Francois and Barriuso, Adela and Torralba, Antonio and Fidler, Sanja},
  booktitle={CVPR},
  year={2021}
}
</code></pre>

 <a href="https://nv-tlabs.github.io/semanticGAN/">SemanticGAN</a>

      <pre><code>@inproceedings{semanticGAN, 
title={Semantic Segmentation with Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization}, 
booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)}, 
author={Li, Daiqing and Yang, Junlin and Kreis, Karsten and Torralba, Antonio and Fidler, Sanja}, 
year={2021}, 
}
</code></pre>

<br>
</section>


<hr>
<hr>


</body>
</html>
